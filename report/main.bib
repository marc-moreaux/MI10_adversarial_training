

@article{rosenblatt1958perceptron,
	title={The perceptron: a probabilistic model for information storage and organization in the brain.},
	author={Rosenblatt, Frank},
	journal={Psychological review},
	volume={65},
	number={6},
	pages={386},
	year={1958},
	publisher={American Psychological Association}
}

@article{goodfellow2014explaining,
	title={Explaining and Harnessing Adversarial Examples},
	author={Goodfellow, Ian J and Shlens, Jonathon and Szegedy, Christian},
	journal={arXiv preprint arXiv:1412.6572},
	year={2014}
}

@webpage{lecunmnist,
	author = {LeCun, Yann and Cortes, Corinna},
	biburl = {http://www.bibsonomy.org/bibtex/25c6723dcce8057a5a41a5d7e12684930/mhwombat},
	howpublished = {http://yann.lecun.com/exdb/mnist/},
	keywords = {MSc _checked character_recognition mnist network neural},
	title = {{MNIST} handwritten digit database},
	url = {http://yann.lecun.com/exdb/mnist/},
	year = 1998
}

@article{krizhevsky2009learning,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey},
  journal={Computer Science Department, University of Toronto, Tech. Rep},
  volume={1},
  number={4},
  pages={7},
  year={2009},
  publisher={Citeseer}
}

@misc{elkan2013maximum,
  title={Maximum Likelihood, Logistic Regression, and Stochastic Gradient Training},
  author={Elkan, Charles},
  year={2013},
  publisher={Tech. Np}
}

@article{hornik1989multilayer,
  title={Multilayer feedforward networks are universal approximators},
  author={Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
  journal={Neural networks},
  volume={2},
  number={5},
  pages={359--366},
  year={1989},
  publisher={Elsevier}
}

@book{bishop2006pattern,
  title={Pattern recognition and machine learning},
  author={Bishop, Christopher M and others},
  volume={4},
  number={4},
  year={2006},
  publisher={springer New York}
}

@ARTICLE{pylearn2_arxiv_2013,
    author = {Goodfellow, Ian J. and Warde-Farley, David and Lamblin, Pascal and Dumoulin, Vincent and Mirza, Mehdi and Pascanu, Razvan and Bergstra, James and Bastien, Fr{\'{e}}d{\'{e}}ric and Bengio, Yoshua},
     title = {Pylearn2: a machine learning research library},
   journal = {arXiv preprint arXiv:1308.4214},
      year = {2013},
       url = {http://arxiv.org/abs/1308.4214},
  abstract = {Pylearn2 is a machine learning research library. This does not just mean that it is a collection of machine learning algorithms that share a common API; it means that it has been designed for flexibility and extensibility in order to facilitate research projects that involve new or unusual use cases. In this paper we give a brief history of the library, an overview of its basic philosophy, a summary of the library's architecture, and a description of how the Pylearn2 community functions socially.}
}

@INPROCEEDINGS{bergstra+al:2010-scipy,
     author = {Bergstra, James and Breuleux, Olivier and Bastien, Fr{\'{e}}d{\'{e}}ric and Lamblin, Pascal and Pascanu, Razvan and Desjardins, Guillaume and Turian, Joseph and Warde-Farley, David and Bengio, Yoshua},
      month = jun,
      title = {Theano: a {CPU} and {GPU} Math Expression Compiler},
  booktitle = {Proceedings of the Python for Scientific Computing Conference ({SciPy})},
       year = {2010},
   location = {Austin, TX},
       note = {Oral Presentation},
   abstract = {Theano is a compiler for mathematical expressions in Python that combines the convenience of NumPy’s syntax with the speed of optimized native machine language. The user composes mathematical expressions in a high-level description that mimics NumPy’s syntax and semantics, while being statically typed and
functional (as opposed to imperative). These expressions allow Theano to provide symbolic differentiation. Before performing computation, Theano optimizes the choice of expressions, translates
them into C++ (or CUDA for GPU), compiles them into dynamically loaded Python modules, all automatically. Common machine learning algorithms implemented with Theano are from 1.6× to 7.5× faster than competitive alternatives (including those implemented with C/C++, NumPy/SciPy and MATLAB) when compiled for the
CPU and between 6.5× and 44× faster when compiled for the GPU. This paper illustrates how to use Theano, outlines the scope of the compiler, provides benchmarks on both CPU and GPU processors, and explains its overall design.}
}

